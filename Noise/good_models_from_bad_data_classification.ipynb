{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3d6db10c-a856-48f9-9f12-c4230643667a",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Building Powerful BioML Models with Massive Label Noise: A Peptide Engineering Tutorial\n",
    "\n",
    "This tutorial demonstrates how machine learning models can remain effective even when \n",
    "trained on datasets with substantial label noise (up to 45%). We explore the counter-\n",
    "intuitive finding that proper hyperparameter adjustment can enable models to extract \n",
    "meaningful biological signals from noisy data.\n",
    "\n",
    "Key findings:\n",
    "- Models trained with 45% label noise can perform nearly as well as those trained on clean data\n",
    "- Larger batch sizes help average out noise effects\n",
    "- The biological signal structure in peptide data provides robustness against label corruption\n",
    "\n",
    "Author: Fatma Elzahraa Eid @ TheBioMLClinic\n",
    "Based on: \"Deep Learning is Robust to Massive Label Noise\" (Rolnick et al., 2017)\n",
    "Dataset: The 12-mer 250K peptide classification dataset in this tutorial is a proprietary dataset of the author. Provided under CC0 license. \n",
    "Code license: MIT\n",
    "email: fatma . elzahraa . eid . work @ gmail . com \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5052cbe-e7de-477b-afa6-6078bf1d5dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6a4eab-ebc2-4789-be4c-64ef76177ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PEPTIDE ENCODING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Standard amino acid alphabet\n",
    "AA_ALPHABET = 'ARNDCQEGHILKMFPSTWYV'\n",
    "\n",
    "def encode_peptide_onehot(sequence):\n",
    "    \"\"\"\n",
    "    Convert a peptide sequence to one-hot encoding.\n",
    "    \n",
    "    Args:\n",
    "        sequence (str): Peptide sequence using single-letter amino acid codes\n",
    "        \n",
    "    Returns:\n",
    "        list: One-hot encoded sequence as list of lists, shape (length, 20)\n",
    "    \"\"\"\n",
    "    char_to_idx = {aa: i for i, aa in enumerate(AA_ALPHABET)}\n",
    "    encoding = []\n",
    "    \n",
    "    for amino_acid in sequence:\n",
    "        # Create zero vector and set appropriate position to 1\n",
    "        onehot_vector = [0] * len(AA_ALPHABET)\n",
    "        if amino_acid in char_to_idx:\n",
    "            onehot_vector[char_to_idx[amino_acid]] = 1\n",
    "        encoding.append(onehot_vector)\n",
    "    \n",
    "    return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc2c3eb-e7f0-433c-9650-ff9e04f04b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CUSTOM EARLY STOPPING CALLBACK\n",
    "# =============================================================================\n",
    "\n",
    "class ProportionalEarlyStopping(Callback):\n",
    "    \"\"\"\n",
    "    Early stopping based on train/validation loss ratio.\n",
    "    \n",
    "    Stops training when (train_loss / val_loss) <= ratio for consecutive epochs.\n",
    "    This helps prevent overfitting to noisy labels by monitoring the relationship\n",
    "    between training and validation performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ratio=0.9, patience=3, verbose=0, restore_best_weights=True):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_loss = logs.get('loss')\n",
    "        val_loss = logs.get('val_loss')\n",
    "        \n",
    "        if train_loss is None or val_loss is None:\n",
    "            return\n",
    "        \n",
    "        # Check if training is overfitting (train loss much lower than val loss)\n",
    "        if (train_loss / val_loss) > self.ratio:\n",
    "            self.wait = 0  # Reset counter\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0 and self.verbose > 0:\n",
    "            print(f'Early stopping at epoch {self.stopped_epoch + 1}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4c18d85-8963-4e14-bde5-0f61d5cb2908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "def create_peptide_lstm_classifier(sequence_length=12, lstm_units_1=140, \n",
    "                                 lstm_units_2=20, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Create LSTM model for peptide fitness classification.\n",
    "    \n",
    "    Args:\n",
    "        sequence_length (int): Length of input peptide sequences\n",
    "        lstm_units_1 (int): Units in first LSTM layer\n",
    "        lstm_units_2 (int): Units in second LSTM layer  \n",
    "        learning_rate (float): Learning rate for Adam optimizer\n",
    "        \n",
    "    Returns:\n",
    "        keras.Model: Compiled LSTM classifier\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(sequence_length, 20)),  # 20 amino acids one-hot encoded\n",
    "        LSTM(lstm_units_1, return_sequences=True, name='lstm_1'),\n",
    "        LSTM(lstm_units_2, return_sequences=False, name='lstm_2'),\n",
    "        Dense(1, activation='sigmoid', name='output')  # Binary classification\n",
    "    ])\n",
    "    \n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f4a7111-87a0-46ab-9fb6-f4351f0575d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_preprocess_data(csv_file, test_size=0.05, random_state=42):\n",
    "    \"\"\"\n",
    "    Load peptide dataset and prepare for training.\n",
    "    \n",
    "    Args:\n",
    "        csv_file (str): Path to CSV file with 'peptide' and 'label' columns\n",
    "        test_size (float): Fraction of data to use for testing\n",
    "        random_state (int): Random seed for reproducible splits\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X_train_encoded, X_test_encoded, y_train, y_test)\n",
    "    \"\"\"\n",
    "    print(\"Loading peptide dataset...\")\n",
    "    \n",
    "    # Load data - assumes CSV has 'peptide' and 'label' columns\n",
    "    df = pd.read_csv(csv_file, usecols=[\"peptide\", \"label\"])\n",
    "    print(f\"Loaded {len(df)} peptide sequences\")\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['peptide'], df['label'], \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=df['label']  # Maintain class balance\n",
    "    )\n",
    "    \n",
    "    print(\"Encoding peptide sequences...\")\n",
    "    # One-hot encode sequences\n",
    "    X_train_encoded = np.array([encode_peptide_onehot(seq) for seq in X_train])\n",
    "    X_test_encoded = np.array([encode_peptide_onehot(seq) for seq in X_test])\n",
    "    \n",
    "    # Convert labels to numpy arrays\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    print(f\"Training set: {len(X_train_encoded)} sequences\")\n",
    "    print(f\"Test set: {len(X_test_encoded)} sequences\")\n",
    "    print(f\"Sequence length: {X_train_encoded.shape[1]}\")\n",
    "    \n",
    "    return X_train_encoded, X_test_encoded, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dea75c3-9308-450f-a7ba-a30c178e8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NOISE INJECTION\n",
    "# =============================================================================\n",
    "\n",
    "def add_label_noise(labels, noise_ratio):\n",
    "    \"\"\"\n",
    "    Add random label noise by flipping a fraction of labels.\n",
    "    \n",
    "    Args:\n",
    "        labels (np.array): Original binary labels\n",
    "        noise_ratio (float): Fraction of labels to flip (0.0 to 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Noisy labels with flipped values\n",
    "    \"\"\"\n",
    "    if noise_ratio <= 0:\n",
    "        return labels.copy()\n",
    "    \n",
    "    noisy_labels = labels.copy()\n",
    "    n_flip = int(len(labels) * noise_ratio)\n",
    "    \n",
    "    # Randomly select indices to flip\n",
    "    flip_indices = np.random.choice(len(labels), size=n_flip, replace=False)\n",
    "    \n",
    "    # Flip selected labels (0->1, 1->0)\n",
    "    noisy_labels[flip_indices] = 1 - noisy_labels[flip_indices]\n",
    "    \n",
    "    return noisy_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d42e3a7-d1a6-4026-94b3-39d88e60f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING EXPERIMENTS\n",
    "# =============================================================================\n",
    "\n",
    "def run_noise_vs_training_size_experiment(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Experiment 1: Effect of label noise at different training set sizes.\n",
    "    \n",
    "    This experiment demonstrates how models perform across varying amounts of\n",
    "    training data and label noise levels.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT 1: Label Noise vs Training Set Size\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Experimental parameters\n",
    "    noise_levels = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45]\n",
    "    training_sizes_k = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # In thousands\n",
    "    \n",
    "    # Batch sizes optimized for each noise level (larger batches for higher noise)\n",
    "    batch_sizes = [16, 32, 64, 128, 256, 256, 512, 512, 1024, 1024]\n",
    "    \n",
    "    learning_rate = 0.001  # Fixed learning rate\n",
    "    \n",
    "    # Store results\n",
    "    results = {noise: {'training_sizes': [], 'accuracies': []} for noise in noise_levels}\n",
    "    \n",
    "    print(f\"Testing {len(noise_levels)} noise levels x {len(training_sizes_k)} training sizes\")\n",
    "    print(\"Noise levels:\", [f\"{int(n*100)}%\" for n in noise_levels])\n",
    "    \n",
    "    for noise_ratio, batch_size_base in zip(noise_levels, batch_sizes):\n",
    "        print(f\"\\nTesting noise level: {int(noise_ratio*100)}%\")\n",
    "        \n",
    "        for train_size_k in training_sizes_k:\n",
    "            current_train_size = train_size_k * 1000\n",
    "            \n",
    "            # Calculate adaptive batch size based on training size\n",
    "            train_ratio = current_train_size / len(X_train)\n",
    "            batch_multiplier = train_ratio * 100\n",
    "            current_batch_size = max(16, int(batch_multiplier * batch_size_base))\n",
    "            \n",
    "            # Sample training data\n",
    "            sample_indices = np.random.choice(\n",
    "                len(X_train), size=current_train_size, replace=False\n",
    "            )\n",
    "            X_train_sample = X_train[sample_indices]\n",
    "            y_train_sample = y_train[sample_indices]\n",
    "            \n",
    "            # Add label noise\n",
    "            y_train_noisy = add_label_noise(y_train_sample, noise_ratio)\n",
    "            \n",
    "            # Train model\n",
    "            model = create_peptide_lstm_classifier(\n",
    "                sequence_length=12, learning_rate=learning_rate\n",
    "            )\n",
    "            \n",
    "            # Train with early stopping\n",
    "            model.fit(\n",
    "                X_train_sample, y_train_noisy,\n",
    "                batch_size=current_batch_size,\n",
    "                epochs=50,\n",
    "                validation_split=0.1,\n",
    "                verbose=0,\n",
    "                callbacks=[ProportionalEarlyStopping(ratio=0.90, patience=3)]\n",
    "            )\n",
    "            \n",
    "            # Evaluate on clean test set\n",
    "            test_predictions = model.predict(X_test, verbose=0)\n",
    "            y_pred_binary = (test_predictions.ravel() >= 0.5).astype(int)\n",
    "            accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "            \n",
    "            # Store results\n",
    "            results[noise_ratio]['training_sizes'].append(train_size_k)\n",
    "            results[noise_ratio]['accuracies'].append(accuracy)\n",
    "            \n",
    "            print(f\"  {train_size_k}K samples, batch={current_batch_size}: {accuracy:.3f}\")\n",
    "    \n",
    "    # Save results\n",
    "    save_results_to_csv(results, 'training_size', 'peptide_noise_vs_training_size_results.csv')\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_noise_vs_batch_size_experiment(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Experiment 2: Effect of batch size at different noise levels.\n",
    "    \n",
    "    This experiment shows how batch size affects model performance when \n",
    "    dealing with label noise.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT 2: Label Noise vs Batch Size\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    noise_levels = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45]\n",
    "    batch_sizes = [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]\n",
    "    \n",
    "    fixed_train_size = 100000  # Fixed training size\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    results = {noise: {'batch_sizes': [], 'accuracies': []} for noise in noise_levels}\n",
    "    \n",
    "    print(f\"Fixed training size: {fixed_train_size:,} samples\")\n",
    "    print(f\"Testing batch sizes: {batch_sizes}\")\n",
    "    \n",
    "    for noise_ratio in noise_levels:\n",
    "        print(f\"\\nTesting noise level: {int(noise_ratio*100)}%\")\n",
    "        \n",
    "        # Sample training data once for this noise level\n",
    "        sample_indices = np.random.choice(\n",
    "            len(X_train), size=fixed_train_size, replace=False\n",
    "        )\n",
    "        X_train_sample = X_train[sample_indices]\n",
    "        y_train_sample = y_train[sample_indices]\n",
    "        y_train_noisy = add_label_noise(y_train_sample, noise_ratio)\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            # Train model\n",
    "            model = create_peptide_lstm_classifier(\n",
    "                sequence_length=12, learning_rate=learning_rate\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_train_sample, y_train_noisy,\n",
    "                batch_size=batch_size,\n",
    "                epochs=50,\n",
    "                validation_split=0.1,\n",
    "                verbose=0,\n",
    "                callbacks=[ProportionalEarlyStopping(ratio=0.90, patience=3)]\n",
    "            )\n",
    "            \n",
    "            # Evaluate\n",
    "            test_predictions = model.predict(X_test, verbose=0)\n",
    "            y_pred_binary = (test_predictions.ravel() >= 0.5).astype(int)\n",
    "            accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "            \n",
    "            results[noise_ratio]['batch_sizes'].append(batch_size)\n",
    "            results[noise_ratio]['accuracies'].append(accuracy)\n",
    "            \n",
    "            print(f\"  Batch size {batch_size}: {accuracy:.3f}\")\n",
    "    \n",
    "    save_results_to_csv(results, 'batch_size', 'peptide_noise_vs_batch_size_results.csv')\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_noise_vs_learning_rate_experiment(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Experiment 3: Effect of learning rate at different noise levels.\n",
    "    \n",
    "    This experiment explores whether increasing learning rates help with\n",
    "    noisy labels, as suggested in the literature.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT 3: Label Noise vs Learning Rate\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    noise_levels = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45]\n",
    "    # Batch sizes increase with noise level\n",
    "    batch_sizes = [128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]\n",
    "    learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "    \n",
    "    fixed_train_size = 100000\n",
    "    \n",
    "    results = {noise: {'learning_rates': [], 'accuracies': []} for noise in noise_levels}\n",
    "    \n",
    "    print(\"Testing how learning rate affects performance with increasing noise\")\n",
    "    print(\"(Batch size also increases with noise level)\")\n",
    "    \n",
    "    for noise_ratio, batch_size in zip(noise_levels, batch_sizes):\n",
    "        print(f\"\\nNoise: {int(noise_ratio*100)}%, Batch size: {batch_size}\")\n",
    "        \n",
    "        # Sample and add noise to training data\n",
    "        sample_indices = np.random.choice(\n",
    "            len(X_train), size=fixed_train_size, replace=False\n",
    "        )\n",
    "        X_train_sample = X_train[sample_indices]  \n",
    "        y_train_sample = y_train[sample_indices]\n",
    "        y_train_noisy = add_label_noise(y_train_sample, noise_ratio)\n",
    "        \n",
    "        for learning_rate in learning_rates:\n",
    "            # Train model\n",
    "            model = create_peptide_lstm_classifier(\n",
    "                sequence_length=12, learning_rate=learning_rate\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_train_sample, y_train_noisy,\n",
    "                batch_size=batch_size,\n",
    "                epochs=50,\n",
    "                validation_split=0.1,\n",
    "                verbose=0,\n",
    "                callbacks=[ProportionalEarlyStopping(ratio=0.90, patience=3)]\n",
    "            )\n",
    "            \n",
    "            # Evaluate\n",
    "            test_predictions = model.predict(X_test, verbose=0)\n",
    "            y_pred_binary = (test_predictions.ravel() >= 0.5).astype(int)\n",
    "            accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "            \n",
    "            results[noise_ratio]['learning_rates'].append(learning_rate)\n",
    "            results[noise_ratio]['accuracies'].append(accuracy)\n",
    "            \n",
    "            print(f\"  LR {learning_rate:.6f}: {accuracy:.3f}\")\n",
    "    \n",
    "    save_results_to_csv(results, 'learning_rate', 'peptide_noise_vs_learning_rate_results.csv')\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3a66cf5-1ee5-4c3f-9a12-056ad73c8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESULTS STORAGE\n",
    "# =============================================================================\n",
    "\n",
    "def save_results_to_csv(results, x_column, filename):\n",
    "    \"\"\"\n",
    "    Save experimental results to CSV format.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Results dictionary from experiments\n",
    "        x_column (str): Name for the x-axis variable column\n",
    "        filename (str): Output CSV filename\n",
    "    \"\"\"\n",
    "    csv_data = []\n",
    "    \n",
    "    for noise_level, data in results.items():\n",
    "        x_values = data[list(data.keys())[0]]  # First key contains x-axis values\n",
    "        accuracies = data['accuracies']\n",
    "        \n",
    "        for x_val, acc in zip(x_values, accuracies):\n",
    "            csv_data.append({\n",
    "                'noise_level': noise_level,\n",
    "                x_column: x_val,\n",
    "                'accuracy': acc\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"\\nResults saved to {filename}\")\n",
    "    print(f\"Total rows: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eca170a-b07f-4dac-97dc-765e936f0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def create_animated_plot(results, x_key, x_label, title, subtitle, filename_base):\n",
    "    \"\"\"\n",
    "    Create animated GIF showing results progressively.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Results from experiments\n",
    "        x_key (str): Key for x-axis data in results dict\n",
    "        x_label (str): Label for x-axis\n",
    "        title (str): Main plot title\n",
    "        subtitle (str): Plot subtitle\n",
    "        filename_base (str): Base filename for outputs\n",
    "    \"\"\"\n",
    "    # Animation parameters\n",
    "    frames_per_point = 3\n",
    "    pause_frames_between_lines = 12\n",
    "    gif_fps = 30\n",
    "    figsize = (10, 6)\n",
    "    \n",
    "    # Set up figure\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Colors for different noise levels\n",
    "    colors = list(plt.get_cmap(\"tab10\").colors) + [\"black\"]\n",
    "    \n",
    "    # Create line objects\n",
    "    noise_levels = list(results.keys())\n",
    "    lines = []\n",
    "    labels = []\n",
    "    \n",
    "    for i, noise in enumerate(noise_levels):\n",
    "        line, = ax.plot([], [], '-o', \n",
    "                       color=colors[i], linewidth=2, markersize=6,\n",
    "                       label=f'Noise = {int(noise * 100)}%')\n",
    "        lines.append(line)\n",
    "        labels.append(\"\")\n",
    "    \n",
    "    # Set up axes\n",
    "    ax.set_xlabel(x_label, fontsize=12)\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    \n",
    "    # Add titles\n",
    "    ax.text(0.5, 1.06, title, ha=\"center\", va=\"bottom\", \n",
    "            transform=ax.transAxes, fontsize=14, weight='bold')\n",
    "    ax.text(0.5, 1.02, subtitle, ha=\"center\", va=\"bottom\",\n",
    "            transform=ax.transAxes, fontsize=11, style=\"italic\")\n",
    "    \n",
    "    # Add signature\n",
    "    ax.text(0.95, 0.05, \"Fatma Elzahraa Eid @ TheBioMLClinic\",\n",
    "            ha=\"right\", va=\"bottom\", transform=ax.transAxes,\n",
    "            fontsize=10, color=\"gray\")\n",
    "    \n",
    "    ax.grid(True, linestyle='--', alpha=0.4)\n",
    "    \n",
    "    # Legend\n",
    "    leg = ax.legend(lines, labels, loc=\"lower left\", frameon=False)\n",
    "    legend_texts = leg.get_texts()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Prepare data arrays\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "    for noise in noise_levels:\n",
    "        x_vals = results[noise][x_key] \n",
    "        y_vals = results[noise]['accuracies']\n",
    "        X_data.append(np.array(x_vals))\n",
    "        Y_data.append(np.array(y_vals))\n",
    "    \n",
    "    # Configure x-axis based on data type\n",
    "    if x_key == 'batch_sizes' or x_key == 'learning_rates':\n",
    "        # Use log scale\n",
    "        min_x = min(np.min(x) for x in X_data if len(x) > 0)\n",
    "        max_x = max(np.max(x) for x in X_data if len(x) > 0)\n",
    "        \n",
    "        if x_key == 'batch_sizes':\n",
    "            ax.set_xscale(\"log\", base=2)\n",
    "            ax.xaxis.set_major_formatter(plt.FuncFormatter(\n",
    "                lambda val, pos: rf\"$2^{{{int(np.log2(val))}}}$\"\n",
    "            ))\n",
    "        else:  # learning_rates\n",
    "            ax.set_xscale(\"log\", base=10) \n",
    "            ax.xaxis.set_major_formatter(mticker.LogFormatterSciNotation())\n",
    "            # Add reference line at 0.001\n",
    "            ax.axvline(x=1e-3, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "            ax.annotate(\"Chosen fixed learning rate\", xy=(1e-3, 0.2), \n",
    "                       xytext=(2e-3, 0.9), \n",
    "                       arrowprops=dict(arrowstyle=\"->\", color=\"black\", lw=1.2),\n",
    "                       ha=\"left\", va=\"center\", fontsize=10)\n",
    "        \n",
    "        ax.set_xlim(min_x * 0.8, max_x * 1.2)\n",
    "    else:\n",
    "        # Linear scale for training sizes\n",
    "        max_x = max(np.max(x) for x in X_data if len(x) > 0)\n",
    "        ax.set_xticks(range(0, int(max_x) + 10, 10))\n",
    "        ax.set_xlim(0, max_x + 5)\n",
    "    \n",
    "    # Animation functions\n",
    "    def init():\n",
    "        for line in lines:\n",
    "            line.set_data([], [])\n",
    "        for text in legend_texts:\n",
    "            text.set_text(\"\")\n",
    "        return lines + legend_texts\n",
    "    \n",
    "    def animate(frame):\n",
    "        # Determine which line to draw and how many points\n",
    "        lines_count = len(noise_levels)\n",
    "        points_per_line = max(len(x) for x in X_data)\n",
    "        total_draw_frames = points_per_line * frames_per_point\n",
    "        frames_per_complete_line = total_draw_frames + pause_frames_between_lines\n",
    "        \n",
    "        current_line = min(frame // frames_per_complete_line, lines_count - 1)\n",
    "        frame_in_line = frame % frames_per_complete_line\n",
    "        \n",
    "        # Draw completed lines\n",
    "        for i in range(current_line):\n",
    "            lines[i].set_data(X_data[i], Y_data[i])\n",
    "            legend_texts[i].set_text(f'Noise = {int(noise_levels[i] * 100)}%')\n",
    "        \n",
    "        # Draw current line progressively  \n",
    "        if frame_in_line < total_draw_frames and current_line < lines_count:\n",
    "            points_to_show = min(\n",
    "                (frame_in_line // frames_per_point) + 1, \n",
    "                len(X_data[current_line])\n",
    "            )\n",
    "            lines[current_line].set_data(\n",
    "                X_data[current_line][:points_to_show], \n",
    "                Y_data[current_line][:points_to_show]\n",
    "            )\n",
    "            legend_texts[current_line].set_text(f'Noise = {int(noise_levels[current_line] * 100)}%')\n",
    "        elif current_line < lines_count:\n",
    "            lines[current_line].set_data(X_data[current_line], Y_data[current_line])\n",
    "            legend_texts[current_line].set_text(f'Noise = {int(noise_levels[current_line] * 100)}%')\n",
    "        \n",
    "        return lines + legend_texts\n",
    "    \n",
    "    # Create animation\n",
    "    total_frames = len(noise_levels) * (max(len(x) for x in X_data) * frames_per_point + pause_frames_between_lines)\n",
    "    anim = FuncAnimation(fig, animate, init_func=init, frames=total_frames, \n",
    "                        interval=1000//gif_fps, blit=True, repeat=True)\n",
    "    \n",
    "    # Save animation and static plot\n",
    "    gif_filename = f\"{filename_base}.gif\"\n",
    "    png_filename = f\"{filename_base}.png\"\n",
    "    \n",
    "    writer = PillowWriter(fps=gif_fps)\n",
    "    anim.save(gif_filename, writer=writer)\n",
    "    print(f\"Animated plot saved as {gif_filename}\")\n",
    "    \n",
    "    # Save final frame as static image\n",
    "    animate(total_frames - 1)\n",
    "    for artist in lines + legend_texts:\n",
    "        artist.set_animated(False)\n",
    "    fig.canvas.draw()\n",
    "    fig.savefig(png_filename, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Static plot saved as {png_filename}\")\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1477845b-5c4f-40bd-9e7c-97227a77af99",
   "metadata": {},
   "source": [
    "### =============================================================================\n",
    "### MAIN EXECUTION\n",
    "### =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c6e39f2-f140-4c1b-be72-446b0617dd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PEPTIDE ENGINEERING WITH LABEL NOISE - TUTORIAL\n",
      "================================================================================\n",
      "\n",
      "This tutorial demonstrates how ML models can handle massive label noise\n",
      "in biological data, specifically peptide fitness prediction.\n",
      "\n",
      "Key concepts:\n",
      "- Label noise robustness through proper hyperparameter tuning\n",
      "- Batch size effects on noisy training\n",
      "- Learning rate considerations for corrupted labels\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 1: DATA LOADING AND PREPROCESSING\n",
      "--------------------------------------------------\n",
      "Loading peptide dataset...\n",
      "Loaded 250000 peptide sequences\n",
      "Encoding peptide sequences...\n",
      "Training set: 237500 sequences\n",
      "Test set: 12500 sequences\n",
      "Sequence length: 12\n",
      "Data loaded successfully!\n",
      "Training sequences shape: (237500, 12, 20)\n",
      "Test sequences shape: (12500, 12, 20)\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 2: RUNNING EXPERIMENTS\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT 1: Label Noise vs Training Set Size\n",
      "============================================================\n",
      "Testing 10 noise levels x 10 training sizes\n",
      "Noise levels: ['0%', '5%', '10%', '15%', '20%', '25%', '30%', '35%', '40%', '45%']\n",
      "\n",
      "Testing noise level: 0%\n",
      "  10K samples, batch=67: 0.908\n",
      "  20K samples, batch=134: 0.913\n",
      "  30K samples, batch=202: 0.935\n",
      "  40K samples, batch=269: 0.937\n",
      "  50K samples, batch=336: 0.935\n",
      "  60K samples, batch=404: 0.937\n",
      "  70K samples, batch=471: 0.943\n",
      "  80K samples, batch=538: 0.944\n",
      "  90K samples, batch=606: 0.948\n",
      "  100K samples, batch=673: 0.947\n",
      "\n",
      "Testing noise level: 5%\n",
      "  10K samples, batch=134: 0.897\n",
      "  20K samples, batch=269: 0.911\n",
      "  30K samples, batch=404: 0.917\n",
      "  40K samples, batch=538: 0.933\n",
      "  50K samples, batch=673: 0.936\n",
      "  60K samples, batch=808: 0.936\n",
      "  70K samples, batch=943: 0.939\n",
      "  80K samples, batch=1077: 0.942\n",
      "  90K samples, batch=1212: 0.936\n",
      "  100K samples, batch=1347: 0.947\n",
      "\n",
      "Testing noise level: 10%\n",
      "  10K samples, batch=269: 0.893\n",
      "  20K samples, batch=538: 0.902\n",
      "  30K samples, batch=808: 0.909\n",
      "  40K samples, batch=1077: 0.913\n",
      "  50K samples, batch=1347: 0.920\n",
      "  60K samples, batch=1616: 0.917\n",
      "  70K samples, batch=1886: 0.925\n",
      "  80K samples, batch=2155: 0.925\n",
      "  90K samples, batch=2425: 0.924\n",
      "  100K samples, batch=2694: 0.935\n",
      "\n",
      "Testing noise level: 15%\n",
      "  10K samples, batch=538: 0.894\n",
      "  20K samples, batch=1077: 0.899\n",
      "  30K samples, batch=1616: 0.904\n",
      "  40K samples, batch=2155: 0.911\n",
      "  50K samples, batch=2694: 0.910\n",
      "  60K samples, batch=3233: 0.913\n",
      "  70K samples, batch=3772: 0.914\n",
      "  80K samples, batch=4311: 0.917\n",
      "  90K samples, batch=4850: 0.916\n",
      "  100K samples, batch=5389: 0.920\n",
      "\n",
      "Testing noise level: 20%\n",
      "  10K samples, batch=1077: 0.876\n",
      "  20K samples, batch=2155: 0.902\n",
      "  30K samples, batch=3233: 0.901\n",
      "  40K samples, batch=4311: 0.905\n",
      "  50K samples, batch=5389: 0.908\n",
      "  60K samples, batch=6467: 0.898\n",
      "  70K samples, batch=7545: 0.909\n",
      "  80K samples, batch=8623: 0.905\n",
      "  90K samples, batch=9701: 0.910\n",
      "  100K samples, batch=10778: 0.909\n",
      "\n",
      "Testing noise level: 25%\n",
      "  10K samples, batch=1077: 0.875\n",
      "  20K samples, batch=2155: 0.894\n",
      "  30K samples, batch=3233: 0.889\n",
      "  40K samples, batch=4311: 0.898\n",
      "  50K samples, batch=5389: 0.897\n",
      "  60K samples, batch=6467: 0.900\n",
      "  70K samples, batch=7545: 0.905\n",
      "  80K samples, batch=8623: 0.906\n",
      "  90K samples, batch=9701: 0.903\n",
      "  100K samples, batch=10778: 0.910\n",
      "\n",
      "Testing noise level: 30%\n",
      "  10K samples, batch=2155: 0.863\n",
      "  20K samples, batch=4311: 0.884\n",
      "  30K samples, batch=6467: 0.890\n",
      "  40K samples, batch=8623: 0.881\n",
      "  50K samples, batch=10778: 0.889\n",
      "  60K samples, batch=12934: 0.879\n",
      "  70K samples, batch=15090: 0.891\n",
      "  80K samples, batch=17246: 0.895\n",
      "  90K samples, batch=19402: 0.880\n",
      "  100K samples, batch=21557: 0.895\n",
      "\n",
      "Testing noise level: 35%\n",
      "  10K samples, batch=2155: 0.852\n",
      "  20K samples, batch=4311: 0.873\n",
      "  30K samples, batch=6467: 0.876\n",
      "  40K samples, batch=8623: 0.885\n",
      "  50K samples, batch=10778: 0.883\n",
      "  60K samples, batch=12934: 0.883\n",
      "  70K samples, batch=15090: 0.888\n",
      "  80K samples, batch=17246: 0.887\n",
      "  90K samples, batch=19402: 0.891\n",
      "  100K samples, batch=21557: 0.893\n",
      "\n",
      "Testing noise level: 40%\n",
      "  10K samples, batch=4311: 0.830\n",
      "  20K samples, batch=8623: 0.842\n",
      "  30K samples, batch=12934: 0.864\n",
      "  40K samples, batch=17246: 0.874\n",
      "  50K samples, batch=21557: 0.874\n",
      "  60K samples, batch=25869: 0.874\n",
      "  70K samples, batch=30181: 0.873\n",
      "  80K samples, batch=34492: 0.874\n",
      "  90K samples, batch=38804: 0.871\n",
      "  100K samples, batch=43115: 0.879\n",
      "\n",
      "Testing noise level: 45%\n",
      "  10K samples, batch=4311: 0.704\n",
      "  20K samples, batch=8623: 0.769\n",
      "  30K samples, batch=12934: 0.790\n",
      "  40K samples, batch=17246: 0.800\n",
      "  50K samples, batch=21557: 0.839\n",
      "  60K samples, batch=25869: 0.809\n",
      "  70K samples, batch=30181: 0.828\n",
      "  80K samples, batch=34492: 0.867\n",
      "  90K samples, batch=38804: 0.839\n",
      "  100K samples, batch=43115: 0.870\n",
      "\n",
      "Results saved to peptide_noise_vs_training_size_results.csv\n",
      "Total rows: 100\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT 2: Label Noise vs Batch Size\n",
      "============================================================\n",
      "Fixed training size: 100,000 samples\n",
      "Testing batch sizes: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]\n",
      "\n",
      "Testing noise level: 0%\n",
      "  Batch size 16: 0.963\n",
      "  Batch size 32: 0.962\n",
      "  Batch size 64: 0.966\n",
      "  Batch size 128: 0.964\n",
      "  Batch size 256: 0.966\n",
      "  Batch size 512: 0.952\n",
      "  Batch size 1024: 0.950\n",
      "  Batch size 2048: 0.942\n",
      "  Batch size 4096: 0.930\n",
      "  Batch size 8192: 0.915\n",
      "\n",
      "Testing noise level: 5%\n",
      "  Batch size 16: 0.959\n",
      "  Batch size 32: 0.960\n",
      "  Batch size 64: 0.958\n",
      "  Batch size 128: 0.959\n",
      "  Batch size 256: 0.958\n",
      "  Batch size 512: 0.951\n",
      "  Batch size 1024: 0.947\n",
      "  Batch size 2048: 0.929\n",
      "  Batch size 4096: 0.919\n",
      "  Batch size 8192: 0.914\n",
      "\n",
      "Testing noise level: 10%\n",
      "  Batch size 16: 0.946\n",
      "  Batch size 32: 0.949\n",
      "  Batch size 64: 0.953\n",
      "  Batch size 128: 0.941\n",
      "  Batch size 256: 0.947\n",
      "  Batch size 512: 0.940\n",
      "  Batch size 1024: 0.941\n",
      "  Batch size 2048: 0.937\n",
      "  Batch size 4096: 0.916\n",
      "  Batch size 8192: 0.916\n",
      "\n",
      "Testing noise level: 15%\n",
      "  Batch size 16: 0.939\n",
      "  Batch size 32: 0.939\n",
      "  Batch size 64: 0.943\n",
      "  Batch size 128: 0.940\n",
      "  Batch size 256: 0.939\n",
      "  Batch size 512: 0.932\n",
      "  Batch size 1024: 0.936\n",
      "  Batch size 2048: 0.922\n",
      "  Batch size 4096: 0.921\n",
      "  Batch size 8192: 0.916\n",
      "\n",
      "Testing noise level: 20%\n",
      "  Batch size 16: 0.918\n",
      "  Batch size 32: 0.926\n",
      "  Batch size 64: 0.921\n",
      "  Batch size 128: 0.925\n",
      "  Batch size 256: 0.930\n",
      "  Batch size 512: 0.925\n",
      "  Batch size 1024: 0.922\n",
      "  Batch size 2048: 0.922\n",
      "  Batch size 4096: 0.913\n",
      "  Batch size 8192: 0.913\n",
      "\n",
      "Testing noise level: 25%\n",
      "  Batch size 16: 0.888\n",
      "  Batch size 32: 0.892\n",
      "  Batch size 64: 0.894\n",
      "  Batch size 128: 0.909\n",
      "  Batch size 256: 0.893\n",
      "  Batch size 512: 0.897\n",
      "  Batch size 1024: 0.896\n",
      "  Batch size 2048: 0.907\n",
      "  Batch size 4096: 0.911\n",
      "  Batch size 8192: 0.910\n",
      "\n",
      "Testing noise level: 30%\n",
      "  Batch size 16: 0.867\n",
      "  Batch size 32: 0.865\n",
      "  Batch size 64: 0.863\n",
      "  Batch size 128: 0.871\n",
      "  Batch size 256: 0.852\n",
      "  Batch size 512: 0.862\n",
      "  Batch size 1024: 0.852\n",
      "  Batch size 2048: 0.899\n",
      "  Batch size 4096: 0.904\n",
      "  Batch size 8192: 0.908\n",
      "\n",
      "Testing noise level: 35%\n",
      "  Batch size 16: 0.751\n",
      "  Batch size 32: 0.776\n",
      "  Batch size 64: 0.767\n",
      "  Batch size 128: 0.769\n",
      "  Batch size 256: 0.786\n",
      "  Batch size 512: 0.787\n",
      "  Batch size 1024: 0.796\n",
      "  Batch size 2048: 0.855\n",
      "  Batch size 4096: 0.885\n",
      "  Batch size 8192: 0.900\n",
      "\n",
      "Testing noise level: 40%\n",
      "  Batch size 16: 0.682\n",
      "  Batch size 32: 0.678\n",
      "  Batch size 64: 0.651\n",
      "  Batch size 128: 0.686\n",
      "  Batch size 256: 0.672\n",
      "  Batch size 512: 0.688\n",
      "  Batch size 1024: 0.697\n",
      "  Batch size 2048: 0.731\n",
      "  Batch size 4096: 0.814\n",
      "  Batch size 8192: 0.881\n",
      "\n",
      "Testing noise level: 45%\n",
      "  Batch size 16: 0.637\n",
      "  Batch size 32: 0.636\n",
      "  Batch size 64: 0.594\n",
      "  Batch size 128: 0.611\n",
      "  Batch size 256: 0.593\n",
      "  Batch size 512: 0.601\n",
      "  Batch size 1024: 0.604\n",
      "  Batch size 2048: 0.611\n",
      "  Batch size 4096: 0.712\n",
      "  Batch size 8192: 0.825\n",
      "\n",
      "Results saved to peptide_noise_vs_batch_size_results.csv\n",
      "Total rows: 100\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT 3: Label Noise vs Learning Rate\n",
      "============================================================\n",
      "Testing how learning rate affects performance with increasing noise\n",
      "(Batch size also increases with noise level)\n",
      "\n",
      "Noise: 0%, Batch size: 128\n",
      "  LR 0.100000: 0.500\n",
      "  LR 0.010000: 0.958\n",
      "  LR 0.001000: 0.964\n",
      "  LR 0.000100: 0.923\n",
      "  LR 0.000010: 0.900\n",
      "  LR 0.000001: 0.840\n",
      "\n",
      "Noise: 5%, Batch size: 256\n",
      "  LR 0.100000: 0.500\n",
      "  LR 0.010000: 0.949\n",
      "  LR 0.001000: 0.950\n",
      "  LR 0.000100: 0.915\n",
      "  LR 0.000010: 0.891\n",
      "  LR 0.000001: 0.776\n",
      "\n",
      "Noise: 10%, Batch size: 512\n",
      "  LR 0.100000: 0.500\n",
      "  LR 0.010000: 0.940\n",
      "  LR 0.001000: 0.958\n",
      "  LR 0.000100: 0.915\n",
      "  LR 0.000010: 0.883\n",
      "  LR 0.000001: 0.773\n",
      "\n",
      "Noise: 15%, Batch size: 1024\n",
      "  LR 0.100000: 0.500\n",
      "  LR 0.010000: 0.926\n",
      "  LR 0.001000: 0.930\n",
      "  LR 0.000100: 0.906\n",
      "  LR 0.000010: 0.878\n",
      "  LR 0.000001: 0.789\n",
      "\n",
      "Noise: 20%, Batch size: 2048\n",
      "  LR 0.100000: 0.500\n",
      "  LR 0.010000: 0.922\n",
      "  LR 0.001000: 0.925\n",
      "  LR 0.000100: 0.895\n",
      "  LR 0.000010: 0.864\n",
      "  LR 0.000001: 0.623\n",
      "\n",
      "Noise: 25%, Batch size: 4096\n",
      "  LR 0.100000: 0.500\n",
      "  LR 0.010000: 0.882\n",
      "  LR 0.001000: 0.912\n",
      "  LR 0.000100: 0.889\n",
      "  LR 0.000010: 0.797\n",
      "  LR 0.000001: 0.450\n",
      "\n",
      "Noise: 30%, Batch size: 8192\n",
      "  LR 0.100000: 0.890\n",
      "  LR 0.010000: 0.870\n",
      "  LR 0.001000: 0.896\n",
      "  LR 0.000100: 0.884\n",
      "  LR 0.000010: 0.790\n",
      "  LR 0.000001: 0.479\n",
      "\n",
      "Noise: 35%, Batch size: 16384\n",
      "  LR 0.100000: 0.500\n",
      "  LR 0.010000: 0.764\n",
      "  LR 0.001000: 0.897\n",
      "  LR 0.000100: 0.874\n",
      "  LR 0.000010: 0.754\n",
      "  LR 0.000001: 0.518\n",
      "\n",
      "Noise: 40%, Batch size: 32768\n",
      "  LR 0.100000: 0.852\n",
      "  LR 0.010000: 0.832\n",
      "  LR 0.001000: 0.880\n",
      "  LR 0.000100: 0.842\n",
      "  LR 0.000010: 0.540\n",
      "  LR 0.000001: 0.514\n",
      "\n",
      "Noise: 45%, Batch size: 65536\n",
      "  LR 0.100000: 0.500\n",
      "  LR 0.010000: 0.829\n",
      "  LR 0.001000: 0.858\n",
      "  LR 0.000100: 0.837\n",
      "  LR 0.000010: 0.688\n",
      "  LR 0.000001: 0.598\n",
      "\n",
      "Results saved to peptide_noise_vs_learning_rate_results.csv\n",
      "Total rows: 60\n",
      "\n",
      "--------------------------------------------------\n",
      "STEP 3: CREATING VISUALIZATIONS\n",
      "--------------------------------------------------\n",
      "Animated plot saved as peptide_noise_vs_training_size.gif\n",
      "Static plot saved as peptide_noise_vs_training_size.png\n",
      "Animated plot saved as peptide_noise_vs_batch_size.gif\n",
      "Static plot saved as peptide_noise_vs_batch_size.png\n",
      "Animated plot saved as peptide_noise_vs_learning_rate.gif\n",
      "Static plot saved as peptide_noise_vs_learning_rate.png\n",
      "\n",
      "================================================================================\n",
      "TUTORIAL COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Key findings:\n",
      "1. Models remain effective even with 45% label noise\n",
      "2. Larger batch sizes help average out noise effects\n",
      "3. Fixed learning rates work well - increasing with noise not necessary\n",
      "\n",
      "Files generated:\n",
      "- 3 CSV files with experimental results\n",
      "- 3 animated GIFs showing progressive results\n",
      "- 3 static PNG plots of final results\n",
      "\n",
      "The biological signal in peptide data provides sufficient structure\n",
      "for models to learn meaningful patterns despite substantial label corruption.\n",
      "\n",
      "This demonstrates that 'garbage in, garbage out' may not apply\n",
      "when the underlying biological signal is strong and structured.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Running the complete tutorial.\n",
    "\n",
    "This cell orchestrates all experiments and creates visualizations\n",
    "demonstrating the robustness of machine learning to label noise in \n",
    "peptide engineering applications.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PEPTIDE ENGINEERING WITH LABEL NOISE - TUTORIAL\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis tutorial demonstrates how ML models can handle massive label noise\")\n",
    "print(\"in biological data, specifically peptide fitness prediction.\")\n",
    "print(\"\\nKey concepts:\")\n",
    "print(\"- Label noise robustness through proper hyperparameter tuning\") \n",
    "print(\"- Batch size effects on noisy training\")\n",
    "print(\"- Learning rate considerations for corrupted labels\")\n",
    "\n",
    "# Load and preprocess data\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"STEP 1: DATA LOADING AND PREPROCESSING\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "csv_filename = \"ThebioMLClinicDatasets_12merPeptideFitness_Classification_250K.csv\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_and_preprocess_data(csv_filename)\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"Training sequences shape: {X_train.shape}\")\n",
    "print(f\"Test sequences shape: {X_test.shape}\")\n",
    "\n",
    "# Run experiments\n",
    "print(\"\\n\" + \"-\"*50)  \n",
    "print(\"STEP 2: RUNNING EXPERIMENTS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Experiment 1: Training size vs noise\n",
    "results_1 = run_noise_vs_training_size_experiment(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Experiment 2: Batch size vs noise  \n",
    "results_2 = run_noise_vs_batch_size_experiment(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Experiment 3: Learning rate vs noise\n",
    "results_3 = run_noise_vs_learning_rate_experiment(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Create visualizations\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"STEP 3: CREATING VISUALIZATIONS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "create_animated_plot(\n",
    "    results_1, 'training_sizes', 'Training Set Size (K)',\n",
    "    'Machine learning is still possible with massive biological label noise',\n",
    "    'Tutorial: 12mer Peptide fitness - Training Size Effect',\n",
    "    'peptide_noise_vs_training_size'\n",
    ")\n",
    "\n",
    "create_animated_plot(\n",
    "    results_2, 'batch_sizes', 'Batch Size', \n",
    "    'For better performance, increase batch size with increasing noise',\n",
    "    'Tutorial: 12mer Peptide fitness - Batch Size Effect',\n",
    "    'peptide_noise_vs_batch_size'\n",
    ")\n",
    "\n",
    "create_animated_plot(\n",
    "    results_3, 'learning_rates', 'Learning Rate',\n",
    "    'Increasing learning rate is not necessarily good with noisy labels', \n",
    "    'Tutorial: 12mer Peptide fitness - Learning Rate Effect',\n",
    "    'peptide_noise_vs_learning_rate'\n",
    ")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TUTORIAL COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKey findings:\")\n",
    "print(\"1. Models remain effective even with 45% label noise\")\n",
    "print(\"2. Larger batch sizes help average out noise effects\")\n",
    "print(\"3. Fixed learning rates work well - increasing with noise not necessary\")\n",
    "print(\"\\nFiles generated:\")\n",
    "print(\"- 3 CSV files with experimental results\")\n",
    "print(\"- 3 animated GIFs showing progressive results\")\n",
    "print(\"- 3 static PNG plots of final results\")\n",
    "\n",
    "print(f\"\\nThe biological signal in peptide data provides sufficient structure\")\n",
    "print(f\"for models to learn meaningful patterns despite substantial label corruption.\")\n",
    "print(f\"\\nThis demonstrates that 'garbage in, garbage out' may not apply\") \n",
    "print(f\"when the underlying biological signal is strong and structured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fbb82d-3a07-47fb-a4a0-afff0f0f2bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc36a3f-ea0b-4205-b25b-88656632b33a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
